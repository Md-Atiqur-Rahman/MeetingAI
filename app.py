import streamlit as st
from streamlit_autorefresh import st_autorefresh
import threading
import time
import numpy as np
import queue
from src.transcriber import transcribe
from src.suggester import get_suggestion
from src.summarizer import generate_summary
from src.audio_listener import start_listening, combined_queue
import json
import os

# File-based communication
TRANSCRIPT_FILE = "temp_transcript.json"

# Global thread control
running_flag = threading.Event()
audio_buffer = []
last_transcription_time = [time.time()]
thread_instance = [None]

def save_transcripts(data):
    """Save transcript data to file"""
    with open(TRANSCRIPT_FILE, 'w') as f:
        json.dump(data, f)

def load_transcripts():
    """Load transcript data from file"""
    if os.path.exists(TRANSCRIPT_FILE):
        try:
            with open(TRANSCRIPT_FILE, 'r') as f:
                return json.load(f)
        except:
            return {"transcripts": [], "latest_text": "", "latest_suggestion": "", "timestamp": time.time()}
    return {"transcripts": [], "latest_text": "", "latest_suggestion": "", "timestamp": time.time()}

def meeting_loop():
    """Background thread for audio processing"""
    global audio_buffer
    
    print("ğŸ™ï¸ Meeting loop started")
    
    listener_thread = threading.Thread(target=start_listening(), daemon=True)
    listener_thread.start()
    
    while running_flag.is_set():
        try:
            if not combined_queue.empty():
                audio_chunk = combined_queue.get(timeout=1)
                audio_np = np.squeeze(audio_chunk).astype(np.float32)

                max_amp = np.max(np.abs(audio_np))
                
                if max_amp > 0.005:
                    audio_buffer.append(audio_np)
                    
                    current_time = time.time()
                    if current_time - last_transcription_time[0] >= 3.0 and len(audio_buffer) > 0:
                        print(f"ğŸ¤ Processing {len(audio_buffer)} audio chunks...")
                        
                        combined_audio = np.concatenate(audio_buffer)
                        audio_buffer = []
                        last_transcription_time[0] = current_time
                        
                        try:
                            text = transcribe(combined_audio)
                            
                            if text and len(text.strip()) > 2:
                                print(f"âœ… Transcribed: {text}")
                                
                                data = load_transcripts()
                                data["transcripts"].append(text)
                                data["latest_text"] = text
                                data["timestamp"] = time.time()
                                
                                try:
                                    suggestion = get_suggestion(text)
                                    if suggestion:
                                        data["latest_suggestion"] = suggestion
                                except Exception as e:
                                    print(f"âš ï¸ Suggestion: {e}")
                                
                                save_transcripts(data)
                                print("ğŸ’¾ Saved")
                            else:
                                print("â­ï¸ No speech")
                                    
                        except Exception as e:
                            print(f"âŒ Error: {e}")
            
            time.sleep(0.1)
            
        except queue.Empty:
            continue
        except Exception as e:
            print(f"âŒ Loop error: {e}")
            time.sleep(0.5)
    
    print("ğŸ›‘ Stopped")

# Initialize session state
if "running" not in st.session_state:
    st.session_state.running = False
if "last_timestamp" not in st.session_state:
    st.session_state.last_timestamp = 0

# Streamlit UI
st.set_page_config(page_title="MeetingAI", layout="wide")

# Custom CSS
st.markdown("""
<style>
    .stButton button {
        height: 50px;
        font-size: 18px;
    }
    @keyframes pulse {
        0%, 100% { opacity: 1; }
        50% { opacity: 0.5; }
    }
    .live-indicator {
        animation: pulse 2s infinite;
        color: #4CAF50;
        font-weight: bold;
    }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ§‘â€ğŸ’¼ MeetingAI - Real-time Meeting Assistant")

col1, col2 = st.columns(2)
with col1:
    start_btn = st.button("â–¶ï¸ Start Meeting", use_container_width=True)
with col2:
    stop_btn = st.button("â¹ï¸ Stop Meeting", use_container_width=True)

caption_box = st.empty()
suggestion_box = st.empty()
summary_box = st.empty()

# Start meeting
if start_btn:
    if not st.session_state.running:
        st.session_state.running = True
        st.session_state.last_timestamp = 0
        
        save_transcripts({"transcripts": [], "latest_text": "", "latest_suggestion": "", "timestamp": time.time()})
        
        if thread_instance[0] is None or not thread_instance[0].is_alive():
            running_flag.set()
            thread_instance[0] = threading.Thread(target=meeting_loop, daemon=True)
            thread_instance[0].start()
            
        st.success("âœ… Meeting started!")
        time.sleep(0.5)
        st.rerun()

# Stop meeting
if stop_btn:
    if st.session_state.running:
        st.session_state.running = False
        running_flag.clear()
        
        st.warning("â¹ï¸ Meeting stopped. Generating summary...")
        
        data = load_transcripts()
        transcripts = data.get("transcripts", [])
        
        if len(transcripts) > 0:
            summary = generate_summary(transcripts)
            summary_box.success(f"ğŸ“ **Meeting Summary:**\n\n{summary}")
        else:
            summary_box.info("No transcript available to summarize.")
        
        # Don't rerun - let user read the summary
        st.info("ğŸ‘† Summary generated! You can now start a new meeting.")

# Main UI
if st.session_state.running:
    
    # AUTO-REFRESH: This runs every 2 seconds
    count = st_autorefresh(interval=2000, key="datarefresh")
    
    # Load data
    data = load_transcripts()
    current_text = data.get("latest_text", "")
    current_suggestion = data.get("latest_suggestion", "")
    all_transcripts = data.get("transcripts", [])
    transcript_count = len(all_transcripts)
    
    # Status bar
    st.markdown(f'<p class="live-indicator">ğŸ”´ LIVE (Auto-refresh: {count})</p>', unsafe_allow_html=True)
    st.caption(f"ğŸ“Š Segments: {transcript_count}")
    
    # Latest caption
    if current_text:
        caption_box.markdown(
            f"""
            <div style='background-color:#e3f2fd; padding:30px; border-radius:15px; 
                        border-left: 8px solid #2196F3; margin: 20px 0;
                        box-shadow: 0 6px 12px rgba(0,0,0,0.15);
                        transition: all 0.3s ease;'>
                <p style='font-size:16px; color:#1565C0; margin:0; font-weight:bold; 
                          text-transform: uppercase; letter-spacing: 2px;'>
                    ğŸ’¬ Latest Transcription
                </p>
                <p style='font-size:28px; color:#000; margin-top:20px; line-height:1.8; 
                          font-weight:500; font-family: -apple-system, system-ui;'>
                    "{current_text}"
                </p>
                <p style='font-size:13px; color:#666; margin-top:20px; font-style: italic;'>
                    Segment #{transcript_count} â€¢ Just now
                </p>
            </div>
            """,
            unsafe_allow_html=True
        )
    else:
        caption_box.info("ğŸ§ Listening... Waiting for speech...")
    
    # AI Suggestion
    if current_suggestion:
        suggestion_box.markdown(
            f"""
            <div style='background-color:#f3e5f5; padding:25px; border-radius:12px; 
                        border-left: 6px solid #9C27B0; margin: 20px 0;
                        box-shadow: 0 4px 8px rgba(0,0,0,0.1);'>
                <p style='font-size:16px; color:#6A1B9A; margin:0; font-weight:bold;
                          text-transform: uppercase; letter-spacing: 1px;'>
                    ğŸ¤– AI Suggestion
                </p>
                <p style='font-size:19px; color:#333; margin-top:15px; line-height:1.6;'>
                    {current_suggestion}
                </p>
            </div>
            """,
            unsafe_allow_html=True
        )
    
    # Sidebar transcript
    with st.sidebar:
        st.subheader("ğŸ“œ Full Transcript")
        st.metric("Total", transcript_count)
        
        if transcript_count > 0:
            st.markdown("---")
            for i, text in enumerate(reversed(all_transcripts[-10:]), 0):
                idx = transcript_count - i
                with st.container():
                    st.markdown(f"**#{idx}**")
                    st.write(text)
                    st.markdown("---")
        else:
            st.info("No transcripts yet")
    
else:
    caption_box.info("ğŸ‘† Click **'â–¶ï¸ Start Meeting'**")
    
    with st.expander("â„¹ï¸ How to Use", expanded=False):
        st.markdown("""
        **Setup:**
        1. âœ… VB-Cable configured
        2. âœ… Audio playing through CABLE Input
        3. âœ… Click Start Meeting
        4. âœ… Captions appear automatically every 2 seconds!
        
        **For Teams meetings:**
        - Set Teams audio output to CABLE Input
        - Keep this app open during the meeting
        - Click Stop Meeting when done to get full summary
        """)

st.markdown("---")
st.caption("ğŸ™ï¸ MeetingAI â€¢ Real-time transcription with Whisper AI")